\chapter{Background theory}
The formulation of the in-the-middle algorithm provided by \textcite{Wedelin08} is based on the definition of a max-sum problem.
In order to apply this algorithm to general \gls{wcsp}, it is necessary to provide a link between the max-sum problem and problem formulations within the graphical model optimization field, where \gls{wcsp} is one of the more general formulations.
This will be done by first introducing the max-sum problem along with some useful interpretations in other fields, followed by a similar introduction of the \gls{wcsp} family.
Finally, a theoretical link between the two fields will be provided, along with an explicit method of translation between the formulations.

\section{Max-sum problems}
The general max-sum problem is an NP-hard optimization problem with many applications in fields ranging from statistical physics to artificial intelligence and pattern recognition.
In general, constrained optimization problem (crucially, several variants of constraint satisfaction) may be restated as max-sum problems.
Formally, the following definition \parencite[based on that of][\pno~11]{Wedelin13} may be used:
\begin{definition}[Max-sum problem] \label{def:max-sum}
	The max-sum problem is the optimization problem
	\begin{equation*}
		\max*[x] f(x) = \sum_{g_k\in G} g_k(x^k),
	\end{equation*}
	where \(g_k(x^k) \in \R\) are distinct arbitrary functions over \(x^k \subseteq x\).
\end{definition}

The max-sum problem then has three distinct components by which it is defined:
\begin{itemize}
	\item A finite set of \emph{variables} \(X = \{x_1, \dotsc, x_n\}\). Let \(X^k \subseteq X\) denote a specific subset of \(X\), and let \(x^k \in X^k\) and \(x \in X\) be \emph{assignments} of the variables.
	\item \emph{Domains} of the variables, \(D = \{D_1, \dotsc, D_n\}\), such that \(x_i \in D_i, \forall i\). Subsets \(D^k \subseteq D\) may be defined analogously to the variable subsets.
	\item A finite set \(G\) of \emph{cost functions} or \emph{components}. Every cost function \(g_k \in G\) is defined over a variable subset \(X^k\), \emph{i.e.} it is a map \(g_k : D^k \mapsto \Rminus \cup \{-\infty\}\).
\end{itemize}
The cost functions may additionally be divided into three kinds: those defined on the empty set (\emph{constants}), those defined on singleton subsets \(X^k = \{x_i\} \subseteq X\) (\emph{variable components}) and those defined on larger subsets (\emph{constraint components}) --- this is the \emph{component model}\label{pg:component-model} introduced by \textcite[\pno~98]{Wedelin08}.

One should also note that the co-domains of \(g_k\) need not be restricted to \(\Rminus \cup \{-\infty\}\), but for the purposes of this thesis that is the chosen output.
This allows solutions \(x\) to the max-sum problem to be ordered by their cost, where the cost is defined as \(\cost{x} = f(x) = \sum_k g_k(x^k)\), and additionally allows the definition of \emph{infeasible} solutions to the max-sum problem as those for which \(\cost{x} = -\infty\).
This will be useful in the translation between \glspl{wcsp} and max-sum problems.

There are several algorithms available for solving max-sum problems, with \textcite{Werner07} mentioning the \emph{augmented DAG algorithm}, the \emph{max-sum diffusion algorithm} and a \gls{lp} relaxation method.
In addition to those direct methods, the relation to \gls{csp} provides many more (which will be mentioned later), and algorithms such as \emph{belief propagation} and \emph{message passing} are also applicable to some max-sum problems.

\subsection{Markov Random Fields}
A restricted variant of the max-sum problem called the \emph{binary max-sum labelling problem} has direct applications to artificial intelligence and pattern recognition, where the problem is known as computing the \gls{map} configuration of \acrlongpl{mrf} \parencite[\pno~1165]{Werner07}.

\section{Weighted constraint satisfaction problems}
\Glspl{csp} are very general decision problems, defined through a set of objects which must satisfy a set of constraints.
Many problems in artificial intelligence and operations research (including planning and resource allocation) may be stated as \glspl{csp}, as well as several academic problems such as \gls{sat}, queens puzzles and map colouring.
One of the corresponding combinatorial optimization problems\footnote{Both the \gls{vcsp} and \gls{scsp} frameworks may be seen as the corresponding optimization problems \parencites{Meseguer06}{Bistarelli99}, but \glspl{wcsp} may be described using either.}, the \gls{wcsp}\footnote{In some literature these problems are called \glspl{cfn}, but the definitions are in essence equivalent.}, additionally introduces \emph{weights} on the constraints, and classify these as \emph{hard} (must be satisfied) or \emph{soft}.
All \glspl{csp} may of course be restated as \glspl{wcsp} with only hard constraints, while the relaxed objective of the combinatorial optimization approach allows even over-constrained \glspl{csp} to be \enquote{solved}.
Additionally, many problems in complexity theory such as \gls{maxsat}, max-clique, max-cut and minimal vertex cover may be modelled using \glspl{wcsp} \parencite[\pno~315]{Meseguer06}.
Due to these facts, \glspl{wcsp} may be regarded as more interesting than \glspl{csp}, especially in an optimization context.

There are several ways to formally define a \gls{wcsp}, but the one used here closely matches the definition of a max-sum problem, which simplifies the formal translation between the two. The definition is based on that presented by \textcite{Meseguer06}, which defines \gls{wcsp} \parencite[\pno~284]{Meseguer06} in terms of a regular \gls{csp} \parencite[\pno~281]{Meseguer06}:
\begin{definition}[\Acrlong{csp}] \label{def:csp}
	A \gls{csp} is a decision problem consisting of three parts:
	\begin{itemize}
		\item A finite set of \emph{variables} \(X = \{x_1, \dotsc, x_n\}\). Let \(V \subseteq X\) denote a specific subset of \(X\).
		\item \emph{Domains} of the variables, \(D = \{D_1, \dotsc, D_n\}\), such that \(x_i \in D_i, \forall i\). Subsets \(D^V \subseteq D\) may be defined analogously to the variable subsets.
		\item A finite set \(C\) of \emph{constraints} \(R_V\in C\) defined by a \emph{relation} \(R\) defined on a subset of variables \(V\subseteq X\), which specify the assignments of \(V\) allowed by the constraint.
	\end{itemize}
	The problem is to find an \emph{assignment} \(t\) which is allowed by all constraints \(R_V\in C\).
\end{definition}

The reformulation as an optimization problem mainly concerns the introduction of \emph{weights}, and a reformulation of the objective:
\begin{definition}[\Acrlong{wcsp}] \label{def:wcsp}
	A \gls{wcsp} (denoted by \textcite[\pno~284]{Meseguer06} as a \emph{k-weighted constraint network}) is a 4-tuple \(\langle X, D, C, k\rangle\), where \(X\) and \(D\) are variables and domains as in \cref{def:csp}, \(C\) is a set of \emph{weighted} constraints and \(k\) is an upper bound.
	A weighted constraint \(f_V\in C\) maps a subset \(V\) of variables to the set \([0,k]\), \emph{i.e.} \(f_V : D^V \mapsto [0,k]\).
	The \emph{cost} of an assignment \(t\) is defined as the (bounded) sum of all \(f_V\), and the optimization problem amounts to
	\begin{equation*}
		\min*[t] \cost{t} = \sum_{f_V\in C} f_V(t^V).
	\end{equation*}
\end{definition}

The attentive reader will notice the similarity between \cref{def:wcsp} and \cref{def:max-sum}.
Using this definition, \emph{feasible} solutions are assignments \(t\) for which \(\cost{t} < k\).
One may also make a distinction between \emph{hard} constraints (\(f_V(t^V) = k\) for some assignment(s) \(t^V\)) and \emph{soft} constraints.

\subsection{Max-CSP and (weighted) max-SAT}
The special case of \gls{wcsp} where all constraints have either unit or zero cost (\emph{i.e.} \(f_V : D^V \mapsto {0,1}\), regardless of choice of \(k\)) is normally referred to as max-\gls{csp} \parencite[\pno~284]{Meseguer06}.
Here, the objective value is exactly the number of violated clauses, and as such it is the most natural formulation of existing \gls{csp} instances as optimization problems.

The special case where all constraints are clauses of a Boolean formula (and weights are unrestricted) yields the \gls{wpms} problem \parencite[\pno~2]{deGivry14}.
If the weights are restricted to unit or zero costs as above, the problem becomes the well-known \gls{maxsat} problem \parencite[\pno~284]{Meseguer06}.

This makes explicit the fact that many real-world and academic problems in satisfiability and operations research may be restated as \gls{wcsp} (and therefore max-sum) problems, and consequently that the in-the-middle algorithm may present a viable alternative to solving these problems using \gls{lp} formulations \parencites{Ansotegui13a}{Davies13} or special algorithms \parencites{Ansotegui13b}{Larrosa08}.

\section{Translating WCSP to max-sum}
The translation from \gls{wcsp} to max-sum, when using the definitions given above, is fairly straight-forward.
In addition to the superficial similarity between \cref{def:wcsp,def:max-sum}, the two problems have deep connections and are in a sense equivalent.
When regarding the \gls{wcsp} formulation as an instance of \gls{scsp} \parencite[\pno~285\psq]{Meseguer06}, the \gls{wcsp} has an associated ordered semiring \(\langle [0,k],+^k,\min*,\leq\rangle\) \parencite[\pno~290]{Meseguer06}.
\Textcite[\pno~1167]{Werner07} noted that the max-sum problem is also associated with a similar ordered semiring structure \(\langle (-\infty,\infty),+,\max*,\geq\rangle\) (although in our case the set is \(\Rminus \cup \{-\infty\}\)).
\Citeauthor{Werner07} additionally provides a connection between max-sum and the regular \gls{csp} through a labelling problem, of which they are both special cases.

In fact, it seems that the only differences between the two problems as stated is the definition of the set included in the semiring --- while the \gls{wcsp} from \cref{def:wcsp} considers a domain \(\{0,\dotsc,k\}\), the max-sum problem according to \cref{def:max-sum} has domain \(\Rminus \cup \{-\infty\}\) --- and the fact that one is a minimization problem while the other is a maximization problem.
However, by transforming the weighted constraints of the \gls{wcsp} problem in such a way that the ordering is preserved but reversed (\emph{i.e.} by negating all costs), and additionally setting \(k=-\infty\) (and changing the costs of all hard constraints accordingly), a formulation which has a semiring \(\langle\Rminus \cup \{-\infty\},+,\max*,\geq\rangle\) is obtained, which is exactly what the max-sum problem has.
From there, it is only a matter of translating the constraints of the \gls{wcsp} into corresponding cost functions in the max-sum formulation.

The translation from \gls{wcsp} to the equivalent max-sum problem may be summarized by a few steps:
\begin{enumerate}
	\item Variable sets and domains are kept in the translation; the sets \(D\), \(X\) and \(X^V\) in \cref{def:wcsp,def:csp} correspond directly to the sets \(D\), \(X\) and \(X^k\) of \cref{def:max-sum}. Additionally, the subsets \(V\) of the \gls{wcsp} problem correspond to the subsets \(k\) in \cref{def:max-sum}.
	\item The weighted constraints \(f_V\) of \cref{def:wcsp} are replaced by new constraints \(f'_V\) defined as
	\begin{equation*}
		f'_V(t) = \begin{cases}
			-\infty &\quad \text{if \(f_V(t) < k\)} \\
			-f_V(t) &\quad \text{otherwise}
		\end{cases}, \quad \forall t.
	\end{equation*}
	\item The cost functions \(g_k\) of the max-sum formulation are constructed from the new constraints, \emph{i.e.} \(g_k = f'_V\), as appropriate.
\end{enumerate}
