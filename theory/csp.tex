% [review] - "cost" is a misnomer (maximizing negative costs? strange.)
\section{(Weighted) Constraint Satisfaction Problems}
Informally, a \gls{csp} may be described as the problem of assigning values to a set of \emph{variables} such that a number of \emph{constraints} are satisfied.
By annotating each constraint of a \gls{csp} with a value denoting the impact of its violation, a \gls{wcsp} is obtained --- the problem becomes to assign values such that the impact is minimized \parencite[\pno~219]{Bistarelli99}.
Each constraint will take different values (\emph{costs}) depending on the variable assignment, and the cost function will depend on these in some unspecified manner (commonly as the sum of all constraint costs).
Such problems are also called \glspl{cfn} in some litterature \parencite{Allouche14}.

\label{page:wcsp-semiring}
A \gls{wcsp}, when defined as a maximization problem, has an associated semiring\footnote{In fact, even regular \glspl{csp} have an associated semiring: \(\langle \{0,1\}, \vee, \wedge, 0, 1 \rangle\).} \(\langle \Rminus\cup\{-\infty\}, \max*, +, -\infty, 0 \rangle\) \parencite[\pno~211]{Bistarelli99} which will be discussed further on.
Such a problem also includes a distinction between \emph{soft} and \emph{hard} constraints.
We will use a formal definition of \gls{wcsp} similar to the one given by \textcite[\pno~3]{Allouche14}\multfootnote{In fact, our definition is equivalent to the triple \(\langle X, W, \infty \rangle\) using the definition given by \textcite{Allouche14}, adapted for maximization instead of minimization.;Note that this definition may also be used to define regular \glspl{csp}, by only including hard constraints.}.

\begin{definition}[\Acrlong{wcsp}] \label{def:wcsp}
	A \gls{wcsp} is a tuple \(\langle X, W \rangle\) where \(X=\{1,\dotsc,n\}\) is a set of \(n\) discrete variables and \(W\) is a set of non-positive functions.
	Each variable \(i \in X\) has a finite domain \(D_i\) of values that can be assigned to it.
	A function \(w_S \in W\), with scope \(S \subseteq X\), is a function \(w_S : D_S \mapsto \{\alpha \in \Rminus \cup \{-\infty\}\}\).
\end{definition}

Using this definition, we can see that \(X\) defines the aforementioned \emph{variables} while \(W\) defines the \emph{constraints}.
Constraints for which \(w_S({x}) = -\infty\) for some input \({x}\) are \emph{hard} constraints, which must be satisfied.
Constraints with \(w_S({x}) < -\infty, \forall {x}\) are \emph{soft} constraints.

Formally, we may now express a solution to the \gls{wcsp} as an assignment \({x} \in D_X\), and we may note that all such solutions have a corresponding cost, defined by \(\cost{{x}} = \sum w_S({x})\).
We may then define a \emph{feasible solution} as a solution \({x}\) that satisfies \(\cost{{x}} > -\infty\).
In some cases we are only interested in finding a feasible solution, but whenever soft constraints are involved it is of interest to find the \emph{optimal solution}, \emph{i.e.} a solution \(\bar{{x}}\) for which \(\cost{\bar{{x}}} \geq \cost{{x}}, \forall {x}\) holds.

% [todo] - how does this affect things?
% [todo] - find a better reference, Bistarelli seems relevant as does Schiex95
\gls{csp}, and as a consequence \gls{wcsp}, is known to be NP-complete \parencite{Mackworth93}.

\subsubsection*{Binary representation of \acrshortpl{wcsp}}
\label{term:direct-encoding}
In order to solve \glspl{wcsp} using the in-the-middle method, the variable set \(X\) has to be transformed --- the in-the-middle algorithm was originally designed to solve binary \gls{lp} instances \parencite{Wedelin95}, and the variant discussed in this thesis is designed to operate on binary representations as well.

The \gls{wcsp} is transformed by \emph{encoding} each variable \(i\in X\) using a more general variation of the \emph{variable component} defined by \textcite[\pno~5]{Wedelin08}.
The encoding may be summarized as follows: for every variable \(i\in X\), introduce \(\abs{D_i}\) binary variables \(z_i^r, r\in D_i\) (and modify the affected constraints \(w_S\) in the obvious manner). 
Then, each variable \(z_i^r\) corresponds to the \(i\)th variable taking the \(r\)th value of its domain \(D_i\).
To ensure that exactly one value from each domain is chosen, additional constraints \(\sum_{r\in D_i} x_i^r = 1, \forall i\) are added --- we will see that these are implicitly enforced by the in-the-middle algorithm.

\subsection{Max-sum formulation of \acrshortpl{wcsp}}
The max-sum problem is most commonly defined as
\begin{equation}
	\label{eq:max-sum-orig}
	\max*[w] f(w) = \sum_k g_k(w^k) + C,
\end{equation}
where \(g_k(w^k) \in \R\) are distinct arbitrary functions over \(w^k \subseteq w\).
Modelling a \gls{wcsp} in terms of a max-sum problem is fairly straight-forward, and will be intstrumental in solving these problems using the in-the-middle algorithm.
Using the fact that max-sum problems have an associated semiring \(\langle\R\cup\{-\infty\},\max*,+,-\infty,0\rangle\) \parencite[\pno~3]{Werner07}, which happens to be the same semiring as that of \glspl{wcsp} (see \cpageref{page:wcsp-semiring}), we can provide a link between max-sum problems and \glspl{wcsp}.

Observing the definitions of max-sum problems and \glspl{wcsp}, it is clear that the set \(S\) corresponds to some \(w^k\) in the max-sum problem definition and that the \(X\) in our \gls{wcsp} corresponds to \(w\) in the max-sum problem.
Next, consider the functions \(w_S\) (\emph{i.e.} all constraints of the \gls{wcsp} defined on the subset \(S\subseteq X\)).
These are in fact equivalent to the \(g_k\) defined in our max-sum problem, with \(g_k(w^k) = w_S(x_S)\), where \(x_S \in D_S \subseteq D\) corresponds to \(w^k \subseteq w\).
For brevity, \(x_S\) may be written as simply \(x\).

All that remains is to relate the objective function of our max-sum problem to the \gls{wcsp}.
We may immediately notice that \(\sum_k g_k(w^k) = \sum w_S(x) = \cost{x}\), and therefore we may express our \gls{wcsp} as the max-sum problem
\begin{equation}
	\label{eq:max-sum-wcsp}
	\max*[x\in D] \cost{x} = \sum_S w_S(x),
\end{equation}
where the constant \(C\) of \eqref{eq:max-sum-orig} is set to \(0\).
As before, we say that a solution \(x\) is \emph{feasible} if \(\cost{x}>-\infty\), and that the problem is feasible if the optimal solution is.

