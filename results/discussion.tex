\section{Discussion}
Analysing the shortcomings of the algorithm it is clear from \cref{tab:comparative-results} that it had significant difficulties in solving the problems from some categories, while it was very successful in others.

The algorithm shows some promise in the \gls{wpms} category.
While most problem sets in this category were omitted due to memory concerns, the algorithm has good runtime performance in the remaining set.
The current implementation represents binary variables of the problem using two distinct variables internally, which could be improved by representing binary variables using only one variable.
This could improve both memory use and runtime, making the algorithm more interesting for approximate applications in the \gls{wpms} field.
The same variable representation issue applies to problems in the \emph{Auction} and \emph{DBN} sets.

One would expect the algorithm to perform well on problems with many hard clauses (due to the sparse representation of constraint components including these implicitly, resulting in fewer clauses to operate on in the constraint update), and this is the case for those sets included in the benchmark.
The algorithm performs well in all sets where a majority of the clauses are hard (\emph{On Call Rostering}, \emph{Parity Learning} and \emph{Pedigree}), but performs worse in sets where only \SIrange{25}{50}{\percent} of the clauses are hard (\emph{Max-Clique}, \emph{Protein Design} and \emph{Linkage}).

The number of zero-value clauses does not seem to have any effect on the performance of the algorithm, suggesting that the tie-breaking method employed efficiently resolves any resulting ties.

Compared to the results of CPLEX, the \gls{lp}-based competitor in the benchmark, the in-the-middle algorithm performs better in most sets.
This indicates that the translation of the algorithm to a max-sum based approach is a better approach than translating max-sum problems to \gls{lp} instances.
The only sets in which CPLEX performs decisively better (those from \gls{cfn}) mainly contain problems of an operations research character (resource allocation, planning \emph{etc.}).

Runtimes could potentially be improved by re-implementing the algorithm using single-precision or integer arithmetic.
The implementation used in the benchmark uses double-precision arithmetic throughout in order to take advantage of the existing framework of the \gls{lp} formulation, for instance when considering sign changes in the modified variable components and when applying noise for tie-breaking.
Single-precision arithmetic could probably be used with little impact on either the implementation or the solution quality achieved by the algorithm.
The problems used in the benchmark all have integer costs (in fact, the Toulbar2 solver appears to work exclusively with integer-valued \glspl{wcsp}), so in theory it would be possible to use and integer arithmetic.
This would however require the code detecting changed variable components to be reimplemented, potentially causing worse performance, and would also require a new tie-breaking mechanism.

% [review] - mention specialized updates?

\subsection{Extensions and improvements}
The performance of the two variants of the algorithms was fairly unexpected, especially with respect to the \enquote{push} operation.
While the optimality gap was expected to decrease, with a corresponding increase in runtime, the opposite happened instead.
The reason for this may be that the algorithm, in trials after the \enquote{push} operation has been applied, is more conservative than the original algorithm in that the maximum \(\kappa\) value will be lower. This means more trials fail to force an integer solution, reducing the number of trials and as a consequence reducing the runtime as well as increasing the optimality gap.
A more correct implementation would take into consideration the reduction of \(\kappa\) caused by the push operation when calculating the maximum \(\kappa\) value of subsequent trials.
% [review] - maybe not true

Due to the results presented above, the \enquote{push} operation is not as interesting when applied to the max-sum algorithm as it is in the original \gls{lp} formulation.
% [review] - analyze why it works on the CFN/Pedigree problem?

The result of the non-fractional \gls{dp} update was more in line with expectations, but nonetheless unintuitive given the mostly insignificant changes in solution quality.
Solution quality was in fact largely maintained while improving runtimes significantly.
The maintained solution quality may be due to the problem instances having very large ranges between trivial upper and lower bounds, with many solutions of similar cost --- this would result in very small changes in the optimality gap, which may not show in the data.
However, the optimality gap is still small in all problem sets, which may be an acceptable compromise given the very low runtimes.

For one problem set (\emph{Auction}) the non-fractional \gls{dp} update showed abysmal performance, failing to solve any instances.
It seems that the reason for this is that the algorithm never reaches a situation where the solution doesn't change between two iterations, instead oscillating between a set of solutions.
This could potentially be resolved by increasing the threshold \(\epsilon\) and amplitude \(\zeta\) of the tie-breaking noise.

With these results in mind, the non-fractional algorithm may be very useful when exact solutions aren't required.
The non-fractional update variant may also be useful as part of a broader strategy, by for instance providing fast and good upper bounds or by providing fast unproven solutions while waiting for exact solvers.
Another interesting direction for the non-fractional \gls{dp} update could be to run it in parallel with the standard algorithm (sharing the immutable constraint components in memory), again providing quick approximate solutions as well as good solutions.
This would also be useful in \emph{e.g.} the \emph{Black Hole} set, where the standard algorithm performed better on easy instances and the non-fractional \gls{dp} update was better on difficult ones.

\subsection{Limitations}
The benchmark has a few limitations which should be mentioned.
For instance, the implementation as it stands has the capability to combine regular \gls{wcsp} constraints with linear (set-partitioning) constraints \parencite[\pno~102]{Wedelin08}, for which more efficient constraint updates may be constructed.
For some problem sets, this could result in runtime improvements.
However, identifying such constraints in a preprocessing stage increases the complexity of the implementation.

Another limitation of the benchmark is the omission of most \gls{wpms} instances.
This is a category where conventional \gls{wcsp} solvers have difficulties due to the large cost function arities, which causes issues with local consistency enforcing.
In theory, in-the-middle algorithm has no such issues with large cost function arities, but the memory requirements for such problems are significant.
Including these problems in the benchmark would likely require better hardware and/or a specialized implementation (specifically, simplifications can be made when solving \gls{wpms} problems due to their binary variable domains).

Finally, the benchmark does not take into consideration the structure of the problems used.
It may well be the case that some problems can be modelled or expressed in a way that makes them easier to solve (for instance, \textcite[\pno~6]{deGivry14} mention that some problem translations are not appropriate for \gls{lp} solvers because they decompose linear constraints).
By tailoring each problem to each solver (for instance, in-the-middle has the ability to combine linear constraints and general max-sum constraints) performance may be significantly improved.
