\section{Implementation}
The algorithm --- including code for reading the \textsc{wcsp} file format \parencite{wcspformat}, parameter sweep strategy and timing facilities --- was implemented in C++11.
The implementation is partly based on an existing framework for the original \gls{lp} in-the-middle algorithm.
The code was compiled using version 5.1 of the LLVM compiler, with all safe optimizations enabled (\emph{i.e.} \texttt{-O3}).

There are several implementation details which are highly relevant to the performance of the algorithm, and this section will explore such details in depth.
In particular, the choice of data structure for constraint component data as well as implementation of constraint updates makes significant impact on the runtime of the algorithm.

\begin{algorithm}[p]
	\SetKwFunction{UpdateConstraint}{UpdateConstraint}
	\Fn{\UpdateConstraint{\(\ccomp_k\), \(\bar{\vcomp}^k\), \(s^k\)}}{
		\KwData{A constraint component \(\ccomp_k\), (pointers to) variable components \(\bar{\vcomp}^k\) and offsets \(s^k\)}
		\KwResult{Updated variable components \(\bar{\vcomp}^k\) and offsets \(s^k\), number of sign changes \(c\)}
		\(c \leftarrow 0\)
		\ForAll{\(\bar{\vcomp}_j\in \bar{\vcomp}^k\)}{
			\(r^k_j \leftarrow \bar{\vcomp}_j - s^k_j\) \;
			\(s^k_j \leftarrow -\infty\) \tct*[r]{Allows omitting \(\ccomp_k(x^k) = -\infty\)}
			\(\vcomp^+_j, \vcomp^-_j \leftarrow -\infty\) \tct*[r]{Used in transforming \(\bar{\vcomp}_j\)}
		}
		\ForAll{pairs \(\langle x^k, \ccomp_k(x^k)\rangle\)}{
			\(v \leftarrow \sum_{x^k_i \in x^k} \vcomp_i(x^k_i)\) \;
			\(v \leftarrow \alpha(v + \ccomp_k(x^k))\) \;
			\ForAll{\(x_j\in x^k\)}{
				\(s^k_j \leftarrow \max{s^k_j, v}\) \;
				\uIf{\(v > \vcomp^+_j\)}{
					\(\vcomp^-_j \leftarrow \vcomp^+_j\) \;
					\(\vcomp^+_j \leftarrow v\) \;
				}
				\ElseIf{\(v > \vcomp^-_j\)}{
					\(\vcomp^-_j \leftarrow v\) \;
				}
			}
		}
		\lForAll(\tct*[r]{Transforms \(\bar{\vcomp}_j\)}){\(\bar{\vcomp}_j\in \bar{\vcomp}^k\)}{
			\(\bar{\vcomp}_j \leftarrow \bar{\vcomp}_j - (\vcomp^+_j + \vcomp^-_j)/2\)
		}
		\ForAll{\(\bar{\vcomp}_j\in \bar{\vcomp}^k\)}{
			Increment \(c\) by \(\#\{\bar{\vcomp}_j : \bar{\vcomp}_j \cdot s^k_j \leq 0\}\) \tct*[r]{Counts number of variable components changed by this update} % [review] - unclear notation?
			\(\bar{\vcomp}_j \leftarrow s^k_j\) \;
			\(s^k_j \leftarrow s^k_j - r^k_j\) \;
		}
		\Return{\(\bar{\vcomp}^k,s^k,c\)}
	}

	\caption{
		Fast implementation of the fractional \gls{dp} update described in \cref{proc:dp-update}.
	}
	\label{proc:frac-dp-update-fast}
\end{algorithm}

Implementation details regarding the parameter sweep strategy presented earlier, which controls the \(\alpha\) parameter of the fractional \gls{dp} update, will also be explained in further detail.

\subsection{Constraint component design decisions}
Several design decisions in the implementation of the constraint components have significant impact on the performance of the constraint component updates.
These design decisions mostly relate to the data structure representing costs inside the constraint component, and the main concern in selecting this data structure is quick access in the update loop.
Since the constraint component is kept constant through all iterations, and the temporary cost table \(h\) can be made implicit, this is the only major concern.

Another implementation detail to note is the storage of the (modified) variable components \(\bar{\vcomp}_i\).
Storing these sequentially in memory is a good choice, but care needs to be taken when ordering the variable components in memory --- due to CPU cache characteristics, storing variable components used in the same constraint component next to each other is highly beneficial.
However, the implementation used here does not reorder variable components in this manner, instead storing them in the order they have been defined in the problem input.

A good data structure for the constraint components \(\ccomp_k\) is a vector with a sparse representation of the table costs (omitting infeasible values, \emph{i.e.} \(\ccomp_k(x^k) = -\infty\)).
These values may be implicitly represented by proper initialization of variables in the constraint update.
The vector representation may be described as a list of pairs \(\langle x^k, \ccomp_k(x^k)\rangle\), where the (fixed) variable values \(x^k\) additionally may be used to access corresponding variable component values \(\bar{\vcomp}_i(x^k_i)\).
In the actual implementation, \(x^k_i\) are represented as pointers to the values \(\bar{\vcomp}_i(x^k_i)\).

\Cref{proc:frac-dp-update-fast} highlights these implementation details, and also shows the use of an invariant transformation of the variable components \(\bar{\vcomp}_i(x^k_i)\) that allows the use of existing code to detect solution changes by counting sign changes.

\subsection{Parameter sweep strategy}
As explained earlier, the fractional \gls{dp} update may be accompanied by a parameter sweep strategy.
Due to the structure of the algorithm implementation, the constraint-specific upper and lower limits of the parameter sweep as described earlier are awkward to handle.
Therefore, these are consolidated into a single parameter \(\kappa\in\left[0,1\right]\).
This is done by mapping \(\kappa\), individually for each constraint, from \(\left[0,1\right]\) to \(\left[\alpha_\perp,\alpha_\top\right]\) using the transformation \(\alpha = \alpha_\perp + \kappa\left(\alpha_\top - \alpha_\perp\right)\).

\begin{figure}[p]
	\centering
	\subfloat[\label{fig:khappa-plot:comp}A max-\gls{csp} problem from the \enquote{Composed} set.]{\input{figs/composed-khappa-plot.tikz}}
	\\
	\subfloat[\label{fig:khappa-plot:deer}A \gls{mrf} problem from the \enquote{Object Detection} set.]{\input{figs/deer-khappa-plot.tikz}}
	\caption{Influence of the \(\kappa\) parameter for two different problems using the fractional update, with noise applied to resolve ties. Only the first trial is shown.}
	\label{fig:khappa-plot}
\end{figure}

Initially, the limits on alpha are \(\alpha_\perp=n^{-1}\) and \(\alpha_\top\) as suggested by theory.
However, the implementation used will make several subsequent attempts at solving the problem; in subequent attempts (\enquote{trials}, futher explained later) the upper limit \(\alpha_\top\) is lowered to match the value at which the prevoius trial found a solution.
This upper limit is implemented in terms of a new upper limit \(\kappa_\top\) on \(\kappa\) (exemplified by \cref{fig:khappa-plot:comp}, for which \(\kappa_\top\) will be roughly \num{0.3} in the next trial).

\Cref{fig:khappa-plot} shows the value of \(\kappa\) for two different problems, along with the number of sign changes which is used as a termination criterion.
In particular, \cref{fig:khappa-plot:deer} shows a full run in which \(\kappa\) is varied throughout the entire range \([0,1]\).
As can be seen, \(\kappa\) is initially kept at 0 for a number of iterations --- attempting to find guaranteed optimal solutions if possible --- after which the parameter is increased fairly quickly until reaching \(0.7\kappa_\top\), after which it is increased more slowly.

\subsubsection{Trials}
As briefly mentioned above, the optimization involves several \emph{trials}.
Before each trial all constraints, variables and costs are reset to their original state.
Then, the parameter sweep is performed and until the final \(\kappa\) value is reached or a feasible solution is found.
If \(\kappa=0\) (\emph{i.e.} the solution is optimal), no more trials are run.
Otherwise, the program moves on to the next trial.

The number of trials is configurable, but the current implementation moves on to a new trial unless the best solution hasn't been improved in the last 4 trials.
