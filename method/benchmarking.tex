\section{Benchmarking}
In order to determine the efficiency of the algorithm, and determine what problem paradigm the algorithm is most usefully applied to, extensive benchmark testing will be performed.
The algorithm was tested against (part of) the large problem set used by \textcite{deGivry14}, which includes problems from the \gls{mrf}, \gls{wpms}, \gls{cfn}, Max-\gls{csp}, \gls{cp} and \gls{cvpr} domains.
All problems in the set are available in the \textsc{wcsp} file format.
Exact data (elapsed time and obtained solution for every solver, as well as proven optima and upper bounds for every problem) for these data sets have been obtained directly from \citeauthor{deGivry14}.

This section will describe the method used to benchmark the algorithm, including the calculation of presented data.
It will also briefly present the problems used in the benchmark, to provide background that may explain the performance characteristics of the algorithm, as well as short introductions to other solvers with which the in-the-middle algorithm will be compared.

\subsection{Benchmarking method}
All problem instances were limited in runtime by the upper time limit \(t_{\text{max}} = \SI{1200}{\second}\), and the benchmarks were run on a single core of an Intel~Core~i5 processor at \SI{2.3}{\giga\hertz}, with \SI{8}{\gibi\byte} RAM.
This is comparable to the conditions of the benchmark performed by \textcite{deGivry14}, and should ensure that the comparisons are valid\footnote{In fact, brief testing with the Toulbar2 solver on the same hardware supports this --- runtimes are of the same order of magnitude as those found by \textcite{deGivry14}.}.
Due to time constraints, data for the comparison solver used in the benchmark was not regenerated on this hardware.

Since the algorithm when used with the corresponding heuristic is an inexact algorithm (while the solvers benchmarked by \textcite{deGivry14} are all exact solvers) the quality of the solution must be compared in addition to the elapsed time per problem.
While the time may be compared as-is (given the comparable hardware conditions), the found solution will have to be compared relative to the proven optimum for each problem.

\label{pg:bench-method}
The relative deviation of the obtained solution (an \emph{optimality gap}) may be calculated as \((f - \bar{f})/(\mathrm{UB}-\mathrm{LB})\), where \(f\) is the solution found by the algorithm, \(\bar{f}\) is the proven optimum and \(\mathrm{UB}, \mathrm{LB}\) is the trivial upper and lower bound of the problem (the upper bound is the sum of the largest cost of each constraint component, and the lower bound is trivially the lowest cost among all constraints).
When \(\bar{f}\) is unknown, the lower bound is used instead.

For the \enquote{push} operation, the constant \(\rho\) was set to \num{5}, the reduction of \(\kappa\) was set to \num{0.8} and the algorithm was run for at most \num{100} additional iterations.
This matches the parameters used by \textcite{Bastert10}.

The stochastic tie-breaking mechanism was included in all benchmark runs, with threshold \(\epsilon=\num{0.01}\) and noise amplitude \(\zeta=1\).
Since all constraint clauses of the problems used in the benchmark have integer costs, having \(\zeta\leq1\) ensures that the tie-breaking noise will never promote tied variables above better, non-tied solutions.

\subsection{Problem sets}
% [review] Wedelin: ...it would be really interesting to look into the modelling of each problem
The problem sets obtained from \textcite{deGivry14} belong to a number of different domains and represent different types of problems from industry, academia and random generation.
This section will briefly review each problem set used in the benchmark, and review both problem source, interpretation and size.
All of these problems are directly representable as \glspl{wcsp}, and hence as max-sum optimization problems, and \textcite{deGivry14} provide details on the translation from each field to the \gls{wcsp} formulation used in this benchmark.

Several sets from the benchmark performed by \textcite{deGivry14} have been omitted from this benchmark.
This is because the in-the-middle algorithm wasn't able to solve any instances in the set (due to time or memory constraints), making them uninteresting in the sense that the algorithm isn't a feasible alternative for those problems.
The omitted problem sets are mostly from the \gls{cp} and \gls{wpms} categories (where only two resp. one problem(s) were kept), but the \emph{Chinese Characters}, \emph{Colour Segmentation}, \emph{Matching Stereo} and \emph{Photo Montage} sets from \gls{cvpr} are also omitted from the benchmark.

It is important to note that the omissions are due to the in-the-middle implementation not being able to solve enough instances for a comparison to be meaningful, and that this will be reflected in the discussion.
In particular, it is already clear that the algorithm has abysmal performance in the \gls{cp} and \gls{wpms} categories.
Comparisons in the \gls{cvpr} category are valid even with the omissions, since the competing solvers show equally poor performance (\emph{i.e.} solve no problems) in the omitted sets \textcite[\pno~9]{deGivry14}.

\subsubsection{Constraint Function Network (CFN)}
This category contains six problem sets, all from the CFLib collection mentioned by \textcite[\pno~3]{deGivry14}.
Most of them are real-world problems or generated to approximate such problems, and all of them are readily available in the \textsc{WCSP} file format mentioned earlier.

\begin{description}
	\item[Auction]
		The combinatorial auction problem has been previously used by \textcites{Larrosa08}{Sandholm99}.
		In summary, the problem allows bidders to bid for indivisible subsets of goods, and the optimization problem is to maximize the revenue of the bid-taker.
		The problems are generated, but inspired by real-world scenarios.
		All variables are binary (the original problem is a binary \gls{maxsat} problem), and the problems contain up to \num{246} variables and \num{12000} constraints.

		The problem set includes \emph{scheduling} and \emph{path} distribution problems, but omits the \emph{regions} distribution mentioned by \textcite[\pno~228]{Larrosa08}.

	\item[CELAR]
		As detailed by \textcite{Cabon99} \parencite[and to some extent][\pno~315\psq]{Meseguer06}, this problem set concerns radio frequency assignment, \emph{i.e.} the problem of providing communication channels from limited resources while minimizing interference in the network.
		The problems where initially introduced in 1993 by \emph{Centre d’Electronique de l’Armement}, and are based on real-world data.

		The CELAR problems are fairly large, with variable domains ranging up to \num{44}, with up to \num{458} variables and \num{2400} constraints.
		Problems included in the benchmark are mainly the CELAR sub-instances \parencite[\pno~85]{Cabon99} and some GRAPH instances.

	\item[Pedigree]
		This category contains problems relating to the Mendelian error correction on complex pedigree \parencites{Sanchez08}[\pno~317\psq]{Meseguer06}, which is a real-world \gls{wcsp}.
		The problem may be described as surveying a pedigree (similar to a family tree), detecting individuals that are erroneous in the sense that they do not conform to the Mendelian laws of inheritance.
		Specifically, the problem formulation is to find the minimum number of errors needed to explain erroneous data.

		The problems are very large, with the number of variables reaching \num{10000} and almost \num{20000} constraints, with variable domains around \num{25}.

	\item[Protein Design]
		\Gls{cpd} problems concern the identification of proteins performing given tasks. The actual problem statement is the optimization of a complex energy function over amino acid sequences, and it is described in length by \textcite{Allouche12}.

		The problems may be expressed by \gls{cfn} or \gls{ilp} models — only the \gls{cfn} formulations are used in this benchmark. The problems contain few (roughly \num{20}) variables with very large domains (up to \num{200}), and around \num{170} constraints.

	\item[SPOT5]
		The SPOT5 problems are in essence planning problems, taken from real-world planning of earth optical observation satellites.
		Given a number of images to be taken during one day using one of three instruments, an associated importance and a set of imperative constraints (transition times, data flow limitations, on-board recording capacity \emph{etc.}), the problem is to find a feasible subset of images that maximize the sum of the associated weights \parencite{Bensana99}.

		The problems are large, with roughly \num{1000} variables and \num{22000} constraints, but the variables are all 4-ary.

	\item[Warehouse]
		Originally presented by \textcite{Kratica01}, the uncapacitated warehouse location instances are randomly generated instances of the facility location problem. In essence, the problem concerns the optimal placement of facilities (in this case warehouses) while minimizing transport costs. These instances were previously used by \textcite{deGivry05} in their evaluation of existential arc consistency for \glspl{csp}.

		These problem instances are very large. The variable domain reaches \num{300} for some problems, with \num{1100} variables and \num{101100} constraint functions.

\end{description}

\subsubsection{Constraint Programming (CP)}
Two problems (one real-world problem and one academic) from this category have been included in this benchmark.
All problems in these sets come from the MiniZinc Challenge\footnote{\url{http://www.minizinc.org/}} \parencite[\pno~5]{deGivry14}, and represent specific problems (representable as \gls{wcsp} instances) defined through \acrlong{cp} languages.
Note that \gls{cp} problems are not generally representable as \glspl{wcsp}, which makes this category less interesting in the context of \gls{wcsp} or max-sum algorithms.

\begin{description}
%	\item[A Maze]
%	\item[Fast Food]
%	\item[Golomb]
	\item[On-call Rostering]
		This problem is a planning problem in which staff members are assigned to days in a rostering period.
		Requirements on staff (both forced rostering and staff being unavailable) affect the schedule, and work load should be even over the rostering period.
		Additionally, staff members are not allowed to be on call more than two days in a row, and prefer not to be on call for two consecutive days.
		Other, similar constraints may also be present.

		Problems in this set are generally fairly large in terms of domain size (up to \num{90}), but only contain up to \num{2200} variables and \num{4500} constraints, which is small compared to other sets.

	\item[Parity Learning]
		The parity learning set contains instances of an optimization variant of the \gls{mdp} problem \parencite{Crawford94}.
		A set of input/output samples of a Boolean function is given, where the function outputs the parity of an unknown subset of the input variables.
		A stated number of the input/output samples are incorrect with respect to the given function, and the goal of the parity learning problem is to find a subset of the input variables that minimizes the number of errors.

		In terms of problem size, instances of this set are fairly small. The number of variables is below \num{760}, and the number of constraints at most \num{1440}. The variable domain sizes are below \num{20}.
%	\item[VRP]
\end{description}

\subsubsection{Computer Vision and Pattern Recognition (CVPR)}
In this category there are nine problem sets containing \gls{mrf} instances from the OpenGM2 benchmark \parencite{Kappes13}.
The problems have been collected from various sources \parencite[\pno~1330]{Kappes13}, but all concern various computer vision tasks performed on real-world images.

The size of these problems vary, with \numrange{20}{500000} variables, \numrange{210}{2000000} constraints and variable domains reaching \num{20} for some sets.

\subsubsection{Max-CSP}
The seven max-\gls{csp} sets are restated binary \gls{csp} instances such that the optimal solution of each instance is the minimal number of unsatisfiable constraints in the original \gls{csp} problem.
The instances are from the 2008 max-\gls{csp} competition\multfootnote{\url{http://www.cril.univ-artois.fr/~lecoutre/benchmarks.html};\url{http://www.cril.univ-artois.fr/CPAI08/}}.
The problems are mostly academic and random, with a relatively small number of variables and constraints (below \num{450} and \num{6500}, respectively).

\begin{description}
	\item[Black Hole]
		\enquote{Black Hole} is a solitaire card game in which card from 17 piles are moved to a centre pile according to certain rules.
		The instances in this set correspond to a simplification of this solitaire game, and were used in the 2005 \gls{csp} Solver Competition.
		\Textcite{Gent07} provide a theoretical background to the \gls{csp} formulation of this problem.
	\item[Colouring]
		In the well-known graph collaring (decision) problem, the objective is to decide whether a given graph is \(k\)-colourable, \emph{i.e.} whether each edge can be assigned one of \(k\) distinct colours such that no adjacent (connected) nodes have the same colour.
		As such, the instances are crafted academic problems.
		The instances in this set originate from the \gls{dimacs}, and have been previously studied by \textcite{Benhamou07}.
	\item[Composed]
		These are random instances composed of an unconstrained \gls{csp} core combined using binary constraints with auxiliary fragments.
		Such problems have been previously used by \textcite{Lecoutre04,Jussien00}.
	\item[EHI]
		The EHI problem instances are random 3-\gls{sat} (\gls{sat} problems where every clause consists of exactly three literals) problems converted into \gls{csp} instances with binary constraints, then further restated as max-\gls{csp} problems. They have previously been considered by \textcite{Lecoutre04}.
	\item[Geometric]
		This set contains problems generated from random points in the unit square. For every pair of points, a hard constraint forbidding the pair is added if they are within a certain distance from each other.
		These instances were used in the 2005 max-\gls{csp} competition \parencite{Boussemart05}.
	\item[Langford]
		The Langford instances are academic instances solving the problem of arranging \(k\) sets of numbers ranging from \(1\) to \(n\) so that appearances of the number \(m\) are exactly \(m\) places apart.
		A general formulation of the problem has been presented by \textcite{Linek03}.
	\item[QCP]
		The \glspl{qcp} are concerned with deciding if a partial Latin square can be filled in order to obtain a full Latin square.
		The set consists of 60 instances previously used in the 2005 \gls{csp} Solver Competition, and the general problem has previously been studied by \textcite{Gomes02}.
\end{description}

\subsubsection{Markov Random Fields (MRF)}
This category consists of seven sets, where the task is to estimate \gls{map} probabilities on \gls{mrf}. The sets represent different underlying problems such as image alignment, genetic linkage analysis, protein folding and other probabilistic problems.

All of these problems, except those in the \emph{Linkage} set (which was used in the UAI'08 probabilistic inference evaluation and later by \textcites{Favier11}{Kishimoto13}), are from the 2011 Probabilistic Inference Challenge\footnote{\url{http://www.cs.huji.ac.il/project/PASCAL/}}.
The problems were translated into their \gls{wcsp} equivalent using a \(-\log{}\) transformation \parencite[\pno~4]{deGivry14}.

Most problems are modest in size, with \numrange{60}{2000} variables and \numrange{1000}{10000} constraints and variable domains below \num{30}.
Some problems have variable domains approaching \num{500}, and some have up to \num{6400} variables and \num{20000} constraints.
Notably, the \emph{Segmentation} set contains both binary and 21-ary formulations of each problem.

\subsubsection{Weighted Partial Max-SAT (WPMS)}
From the field of \gls{wpms}, only one problem set was kept.
Problems in this field contain a very large number of (binary, for obvious reasons) variables and cost functions with very large arity --- in fact, this is the only field in which cost function arity exceeds \num{5} (most other sets have cost functions of two variables only).
The only kept problem set, \emph{Max-Clique}, has a cost function arity of \num{2}.
Discarded sets were omitted due to memory concerns, likely caused by inefficient representation of the cost functions.
The instances were used in the 8th \Gls{maxsat} Evaluation\footnote{\url{http://maxsat.ia.udl.cat/13/benchmarks/}}.

\begin{description}
%	\item[Haplotyping]
	\item[Max-Clique]
		The Max-Clique problem, which may be restated as a \gls{maxsat} problem \parencite{Heras08}, is the well-known problem of finding the largest \emph{clique} (complete subgraph) of a graph.
		It may be regarded as an academic problem, but has many applications to real-world problems and has seen extensive study when it comes to tailored algorithms for the original formulation.
		The original instances are from the second \gls{dimacs} challenge \parencite{Johnson96}, and have been previously used by \emph{e.g.} \textcite{Östergård02} for benchmarking max-clique algorithms.

		When translated into their \gls{maxsat} equivalent, max-clique instances become very large. While the number of variables is fairly low (below \num{3400}), the number of constraints is very large (approaching \num{380000}).
%	\item[MIPLib]
%	\item[Packup Weighted]
%	\item[Planning With Preferences]
%	\item[Timetabling]
%	\item[Upgradeability]
\end{description}

\subsection{Solvers}
Three solvers used by \textcite{deGivry14} are included in this benchmark.
This section will briefly describe their original field of application, give brief pointers to the method they employ, and relate them to the in-the-middle algorithm.

\subsubsection{Toulbar2}
The Toulbar2 solver is an exact anytime solver for \glspl{wcsp} based on a depth-first branch-and-bound algorithm \parencite{Allouche10}.
In addition to using a strong arc consistency property\footnote{Existential directed arc consistency, as presented by \textcite{deGivry05}.}, the solver uses a sizeable bag of tricks including variable elimination, dead-end elimination \parencite{deGivry13} and pairwise decomposition \parencite{Favier11}.

The Toulbar2 solver is by far the most advanced solver in the \gls{wcsp} field, with significant amounts of theory behind it especially with respect to strong arc consistency properties (the one implicitly employed by the in-the-middle algorithm, generalized arc consistency, is quite weak in comparison).
It is therefore an interesting benchmark \enquote{opponent}, and matching it in a benchmark (especially the \gls{mrf}, \gls{cfn} and max-\gls{csp} categories) could indicate potential for the in-the-middle algorithm in those categories.

\subsubsection{CPLEX}
The well-known CPLEX solver, which is an exact \gls{mip} solver rather than a \gls{wcsp} solver, is also included in the benchmark.
Using the \emph{direct} encoding described by \textcite[\pno~3]{deGivry14} of \glspl{wcsp} into 0--1 \gls{ilp} problems, CPLEX was found to have very good performance for some problem categories an therefore it is also included in this benchmark.
CPLEX is highly optimized proprietary software, but uses the well-known simplex and barrier interior point methods to solve \gls{lp} problems.
Even so, previous results \parencite{Mason01,Ernst05} have shown that the \gls{lp} formulation of the in-the-middle algorithm is competitive with CPLEX, and it is therefore interesting to see how the two relate in the \gls{wcsp} field.

\subsubsection{MaxHS}
The MaxHS solver is a \gls{maxsat} and \gls{wpms} solver based on decomposing \gls{maxsat} problems into several smaller \gls{sat} instances, solving these using cooperation between an underlying \gls{sat} solver and a \gls{mip} solver \parencite{Davies11}.
It has been found to perform well in solving non-random \gls{maxsat} instances \parencite{Davies13}, and is therefore a prime candidate for comparison in the \gls{wpms} (and to some extent max-\gls{csp}) categories.
